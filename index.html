<!DOCTYPE html><html lang="zh"> <head><meta charset="UTF-8"><title>sakanaction的博客</title><!-- KaTeX CSS --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><style>*{box-sizing:border-box}body{margin:0;font-family:system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica,Arial,"Apple Color Emoji","Segoe UI Emoji";line-height:1.7;background:#fafafa;color:#222}main{max-width:760px;margin:3rem auto;padding:0 1.2rem}header{margin-bottom:2rem}header a{text-decoration:none;color:#222;font-weight:600}.site-header{border-bottom:1px solid #eee;background:#fff}.nav-container{max-width:960px;margin:0 auto;padding:1rem 1.2rem;display:flex;justify-content:space-between;align-items:center}.logo{font-weight:700;font-size:1.05rem;text-decoration:none;color:#111}.nav-links{display:flex;gap:1.5rem}.nav-links a{font-size:.95rem;text-decoration:none;color:#444;border-bottom:1px solid transparent;transition:color .2s ease,border-color .2s ease}.nav-links a:hover{color:#2563eb;border-bottom-color:currentColor}h1,h2,h3{line-height:1.3}h1{margin-bottom:1.2rem}h2{margin-top:2.5rem}article p{margin:1rem 0}article ul{padding-left:1.4rem}article li{margin:.4rem 0}pre{background:#f4f4f4;padding:1rem;overflow-x:auto;border-radius:6px}code{background:#f4f4f4;padding:.2em .4em;border-radius:4px;font-size:.95em}.katex{font-size:1.05em}a{color:#00040c;text-decoration:none;border-bottom:1px solid transparent;transition:border-color .2s ease,color .2s ease}a:hover{color:#274ecf;border-bottom-color:currentColor}.home-intro{margin-bottom:3rem}.home-intro h1{margin-bottom:.5rem}.home-intro p{color:#555;max-width:600px}.home-posts h2{margin-bottom:1rem;font-size:1.2rem;color:#333}.home-posts ul{list-style:none;padding:0;margin:0}.post-item{display:flex;justify-content:space-between;align-items:baseline;padding:.6rem 0;border-bottom:1px solid #eee}.post-item a{font-weight:500}.post-date{font-size:.85rem;color:#888}.post-title{font-weight:700;font-size:1.2rem}.post-date{display:block;font-size:.9rem;color:#666}.post-abstract{margin-top:.5rem;color:#333}.post-grid{display:grid;gap:1.8rem}.post-card{display:block;padding:1.6rem;border-radius:12px;border:1px solid #e5e7eb;text-decoration:none;color:inherit;background:#fff;transition:all .2s ease}.post-card:hover{transform:translateY(-4px);box-shadow:0 10px 25px #00000014;border-color:#d1d5db}.post-meta{font-size:.85rem;color:#9ca3af}.post-layout{display:grid;grid-template-columns:minmax(0,1fr) 220px;gap:2.5rem;align-items:start}.post-content{min-width:0}.post-toc{position:sticky;top:2rem;font-size:.9rem;color:#444}.post-toc nav{border-left:2px solid #e5e7eb;padding-left:1rem}.post-toc strong{display:block;margin-bottom:.6rem;font-weight:600}.post-toc ul{list-style:none;padding:0;margin:0}.post-toc li{margin:.35rem 0}.post-toc li.depth-3{padding-left:1rem}.post-toc li.depth-4{padding-left:2rem}.post-toc a{color:#555;border-bottom:none}.post-toc a:hover{color:#2563eb}@media(max-width:1024px){.post-layout{grid-template-columns:1fr}.post-toc{position:static;margin-bottom:2rem;order:-1;border-left:none;padding-left:0}.post-toc nav{background:#f3f4f6;padding:1rem;border-radius:8px}}
</style></head> <body> <header class="site-header"> <div class="nav-container"> <a href="/myblog/" class="logo">我的博客</a> <nav class="nav-links"> <a href="/archive">归档</a> <a href="/tags">标签</a> <a href="https://github.com/learning-up-up" target="_blank" rel="noopener">GitHub</a> </nav> </div> </header> <main>  <section class="home-intro"> <h1>sakanaction的博客</h1> <p>
一个人工智能专业本科生的博客，记录学习内容。
</p> </section> <section class="home-posts"> <h2>文章</h2> <div class="post-grid"> <a href="/myblog/posts/post3/"> <h3 class="post-title">决策树</h3> <p class="post-abstract"> 一种基本的分类与回归方法。主要包括三步：特征选择、决策树的生成、决策树的修剪 分类问题是对抑制标签的学习，而聚类才是自行总结特征。 模型 通过树形结构来描述分类的结果，其中内部节点表示特征，叶子节点代表一个类。 从两个角度来看决策树： 每一条根节点到叶子节点的路径表示了一个if-then的逻辑结构，所有的路径构成的集合完备的表示了每一个实例被分类的过程。 从条件概率的角度来看，所有路径过程的集合实现了对于特征空间的划分。决策树刻画了不同实例属于不同类别的条件概率分布。最终由联合概率分布的大小决定不同实例的类别。 学习过程 我们的目标是得到一个可以较好地对于训练数据进行分类描述地决策树，同时还有具备一定的泛化能力。往往通过正则化的损失函数来描述这一目标 递归的选择最优的特征对于特征空间进行分割，直至没有可再选择的特征或者分类已经足够满意。然后对于已有的树进行剪枝，提高其泛化能力 特征选择 用于分类的特征一定要是能对于训练数据进行有效分类的特征。如何评价一个分类是否有效，可以引入信息增益进行描述 信息增益 首先引入熵的概念。在物理中，熵描述了系统的混乱程度，而在概率论与信息论中，熵作为对于随机变量不确定性的度量。 对于随机变量$$，若 则定义 特别的，当$$时，$$由于熵完全由随机变量的分布来决定，熵也可以写作是对于分布的函数。 也可以写作 熵越大，随机变量的不确定性就越大，$$ 条件熵$$表示在Y在X约束下的不确定程度。 有估计（如最大似然估计）得到的熵称为经验熵。 而信息增益就是用来描述特征X使类Y不确定性减少的程度。显然信息增益越大，一个特征的分类效果越好 信息增益，特征D对于训练集A的信息增益定义为 可以解释为引入A特征是否对于特征集的不确定性有有效的降低，评价了分类的有效性，信息增益大的特征具有更强的分类能力 根据信息增益进行特征选择就是选择信息增益最大的特征 信息增益的算法： 对于训练集D，K个类将训练集划分为$$。对于特征A，有n个取值，将训练集划分为n个子集，可以得到： 信息增益比：使用信息增益可能会导致倾向取值较多的特征，可以进行一个归一化 决策树的生成 ID3算法 从根节点开始，对所有的节点计算可能的特征的信息增益，选择信息增益最大的特征作为节点的特征，对于子节点不断递归这一过程直到所有的特征都被选择完为止。 算法过程： 若D中所有实例属于同一类，直接返回该类的标签 若特征集A为空，直接返回D中实例数最大的类的标签 否则计算各特征的信息增益，选择信息增益最大的特征 若小于阈值，则也就构建一个单节点树 否则对于该特征的所有可能值对于训练集分割，将子集中的实例数最大的类别作为标签构建子节点 对于所有子节点递归这一过程 C4.5 使用信息增益比来进行决策树的生成 剪枝 直接训练出来的决策树模型对于训练数据的分类十分准确，但是对测试数据的分类未必会很好，即出现过拟合现象。过拟合现象出现的原因是模型过度的考虑如何适应训练数据而构建出过于复杂的决策树，我们可以考虑对于决策树剪枝来避免这一情况。 可以通过最小化损失函数的方式来实现。损失函数的定义： 其中$$是树T的叶节点。损失函数就是加入了正则项，避免模型过于复杂 剪枝算法： 计算每个结点的经验熵 递归的从树的叶节点向上回缩 比较回缩后的损失函数与回缩前，若损失函数减小，则在此节点处进行剪枝 CART算法 分类与回归树也是一种决策树学习方法，同样有特征选择、树的生成、剪枝构成。CART假设决策树是一个二叉树，所有的节点都是取“是”或“否”，是递归的对于所有的特征进行二分。对于输入的随机变量X，给出Y的条件概率分布的学习结果。… </p> <div class="post-meta"> 2/13/2026 </div> </a><a href="/myblog/posts/post2/"> <h3 class="post-title">k邻近法</h3> <p class="post-abstract"> 一种基本的分类回归方法，其并没有显示的学习过程。对于一个测试实例，通过训练集中距离最近的k个实例来进行表决，得到该实例的分类结果。 模型 k邻近法的核心就是通过对于特征空间的划分，实现决策。其中重点要考虑的是距离度量、k值、表决方式 距离度量可以取p范数（常用2范数） k的取值 k如果过小，那么k值就会更加明显的受到最临近点的影响，倘若最邻近点就是噪声，那么分类结果就会出现较大的偏差。k过小会是模型变得复杂，过拟合的概率增加 k如果过大，考虑极限情况$$，此时无论测试实例为什么，结果都是固定的。此时的模型过于简单 k一般还是去一个较小的值，通过交叉验证法来确定最佳的k值 决策方式 通常使用多数最优的方式，考虑$$损失函数，那么多数最优就是经验风险最小化的情况 kd树 我们往往通过构造一个二叉树来实现对于k维特征空间的划分，称之为kd树，构造kd树的方法非常简单。 实际上我们就是在用垂直于坐标轴的超平面不断地去划分特征空间，步骤如下 1. 取$$为坐标轴，取N个实例中$$为中位数的为根节点，其中小于该中位数的为左子树，其余为右子树 2. 对于深度为$$的节点，取$$为坐标轴（$$），重复的进行划分直到子区域中没有实例点 搜索 1. 首先找到测试点所属的叶子节点：从根结点出发，按照对应坐标轴进行分类就得到此时的最邻近点 2. 回溯：递归回退 1. 若该节点到测试点的距离更近，则更新最邻近点的值 2. 检查另一子树内是否有更邻近的点，即检查以测试点为球心，最近距离为半径的超球体是否与该区域相交，若不相交，向上回退 3. 直到回退到根节点，此时最邻近的点就是结果 对于最邻近的k个点，那么在回溯的过程中就要维护一个大顶堆，通过比较大顶堆堆顶的值来进行更新。 总结 kNN适合小样本的训练，其可解释性较弱。适合一些简单的问题，应用范围不广。 </p> <div class="post-meta"> 2/12/2026 </div> </a><a href="/myblog/posts/post1/"> <h3 class="post-title">感知机</h3> <p class="post-abstract"> 感知机时用于二分类的线性模型，输入为实例的特征向量，输出为类别，取$$。相当于在特征空间中找到一个超平面将空间划分。 模型表示 希望得到一个从输入空间到输出空间的函数 其中 ，$$为权重向量，$$为偏置。（为了简便，向量并未加粗表示） 解释：$$表示超平面，$$决定了超平面的方向，$$决定了超平面到原点的距离。 学习策略 数据集的线性可分性 对于感知机来说，首先要保证模型是线性可分的，及对于一个数据集来说，可以将正是锂电和腐蚀锂电完全正确的划分开来。 损失函数 由于我们要最小化损失函数，因此定义的损失函数尽可能是连续可导函数。在这里可以选择误分类点到超平面的距离之和作为损失函数。 对于任意一点$$，其到超平面的距离为 对于误分类的点$$，有$$，因此可以将损失函数定义为 这里略去了常系数$$，$$为误分类点的集合。 学习算法 问题重述：对于训练集 其中$$，$$，希望通过学习得到参数$$和$$，使得损失函数最小化： 采用随机梯度下降法，首先任选一个超平面，再每次选取一个误分类点使其梯度下降。 损失函数的梯度 每次选择一个误分类点对$$更新，由于梯度方向是增长最快的方向，选取梯度的一个分量 $$表示学习率，这样可以期待损失函数不断减小直到为零 因此感知机学习算法的原始形式： 1. 选取$$ 2. 在训练集中选择数据$$ 3. 若$$则 4. 重复直至没有误分类点 收敛性：若线性可分，就必然收敛记$$，则在训练集上的试错次数$$ 对偶形式 实际上如果$$的初值都取$$，那么可以得到二者的最终形式就是关于$$的线性形式。那么我们一开始就如此来表示我们所需要的目标参数 对比原始的学习算法，我们可以将更新步骤改为 总结 算法迭代的过程本质上是对于一个误分类点，超平面不断地作平移旋转直至将他变为正确分类的点 实例 通过平面上可二分类的两堆点来画出分割的曲线，可以调取机器学习库中的模块 </p> <div class="post-meta"> 2/11/2026 </div> </a> </div> </section>  </main> </body></html>